# ğŸ›’ Airflow ETL Pipeline with Great Expectations

## ğŸ“Œ Project Overview

This project implements an **end-to-end ETL pipeline** using **Apache Airflow**, **Great Expectations**, and **Docker**.  
The pipeline extracts e-commerce data, validates data quality, performs transformations, and loads the processed data while ensuring **data reliability through automated validations**.

The entire system is **containerized** and **production-aligned**, following modern data engineering best practices.

---

## ğŸ— Architecture Overview

### ğŸ” ETL Flow

```text
Extract Data
   â†“
Validate Raw Data (Great Expectations)
   â†“
Transform Data
   â†“
Validate Transformed Data (Great Expectations)
   â†“
Load Data
```

### ğŸ§© Components

- **Airflow Webserver & Scheduler** â€“ Workflow orchestration  
- **ETL Service (Python)** â€“ Transformations & unit tests  
- **Great Expectations** â€“ Data quality validation  
- **PostgreSQL** â€“ Airflow metadata database  
- **Docker Compose** â€“ Service orchestration  

---

## ğŸ§° Tool Stack

| Tool | Purpose |
|----|----|
| Apache Airflow | Workflow orchestration |
| Great Expectations | Data validation |
| Python 3.10 | ETL logic |
| Docker & Docker Compose | Containerization |
| Pytest | Unit testing |
| SQLite | Analytics data storage |
| Git & GitHub | Version control |

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/Chopra-14/airflow-great-expectations-etl.git
cd airflow-great-expectations-etl
```

### 2ï¸âƒ£ Environment Variables

Create `.env.example`:

```env
AIRFLOW_UID=50000
SQLITE_DB_PATH=/data/analytics.db
```

### 3ï¸âƒ£ Start All Services
```bash
docker-compose up -d
```

### 4ï¸âƒ£ Verify Containers
```bash
docker ps
```

Ensure these containers are running:

- airflow_webserver  
- airflow_scheduler  
- etl-service  
- postgres  

---

## ğŸš€ DAG Execution Steps

1. Open Airflow UI â†’ http://localhost:8080  
2. Enable DAG: `ecommerce_analytics_pipeline`  
3. Trigger DAG manually â–¶ï¸  

### âœ… Expected Result

- All tasks **GREEN**
- DAG run status = **SUCCESS**

---

## âœ… DAG Configuration

| Setting | Value |
|------|------|
| Schedule | `@daily` |
| Retries | 2 |
| Retry Delay | 5 minutes |
| Catchup | False |

---

## ğŸ” Validation Strategy (Great Expectations)

### âœ” Raw Data Validation
- Column presence checks  
- Schema consistency  
- Executed via Great Expectations checkpoint  

### âœ” Transformed Data Validation
- Schema integrity checks  
- Data consistency checks  

### âœ” Failure Handling
- DAG fails immediately if validation fails  
- Downstream tasks are blocked  

---

## ğŸ§ª Unit Testing

Run tests inside ETL container:

```bash
docker-compose exec etl-service pytest
```

### Included Tests
- Transformation logic test  
- Schema validation test  

âœ” Passing tests ensure reliable ETL logic

---

## ğŸ—‚ Screenshots (Evidence)

| Screenshot | Description |
|----------|------------|
| step13_01_docker_ps_running.png | All containers running |
| step14_02_airflow_dag_list.png | DAG visible in Airflow |
| step14_03_dag_graph_view.png | DAG graph view |
| step14_04_dag_grid_success.png | All tasks successful |
| step14_05_task_log_success.png | Task log output |
| step14_06_great_expectations_data_docs.png | GE Data Docs |
| step12_01_pytest_success.png | Pytest success |

### â­ Bonus Screenshots
- Great Expectations CLI validation  
- Expectation Suite HTML  
- Data Docs index page  

---

## ğŸ—„ How to Verify SQLite Database

```bash
docker-compose exec etl-service bash
sqlite3 /data/analytics.db
.tables
SELECT * FROM analytics_table LIMIT 5;
```

---

## ğŸ Final Status

âœ” Fully containerized  
âœ” Automated validation implemented  
âœ” Unit test coverage added  
âœ” End-to-end execution verified  
âœ” **Portfolio-ready project**

---

## ğŸ™Œ Author

**Chopra Lakshmi Sathvika**  
Data Engineering | Apache Airflow | Great Expectations | Docker
# ğŸ›’ Airflow ETL Pipeline with Great Expectations

## ğŸ“Œ Project Overview

This project implements an **end-to-end ETL pipeline** using **Apache Airflow**, **Great Expectations**, and **Docker**.  
The pipeline extracts e-commerce data, validates data quality, performs transformations, and loads the processed data while ensuring **data reliability through automated validations**.

The entire system is **containerized** and follows modern **data engineering best practices**.

---

## ğŸ— Architecture Overview

### ğŸ” ETL Flow

```text
Extract Data
   â†“
Validate Raw Data (Great Expectations)
   â†“
Transform Data
   â†“
Validate Transformed Data (Great Expectations)
   â†“
Load Data

```

## ğŸ§© Components
```

- **Airflow Webserver & Scheduler** â€“ Workflow orchestration  
- **ETL Service (Python)** â€“ Data transformation logic & unit tests  
- **Great Expectations** â€“ Data quality validation  
- **PostgreSQL** â€“ Airflow metadata database  
- **Docker Compose** â€“ Service orchestration  
```

## ğŸ§° Tool Stack
```

| Tool | Purpose |
|------|--------|
| Apache Airflow | Workflow orchestration |
| Great Expectations | Data validation |
| Python 3.10 | ETL logic |
| Docker & Docker Compose | Containerization |
| Pytest | Unit testing |
| SQLite | Analytics data storage |
| Git & GitHub | Version control |
```

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/Chopra-14/airflow-great-expectations-etl.git
cd airflow-great-expectations-etl
```
### 2ï¸âƒ£ Environment Variables
```
Create a file named `.env.example`:

```env
AIRFLOW_UID=50000
SQLITE_DB_PATH=/data/analytics.db
```

### 3ï¸âƒ£ Start All Services
```
Start all containers using Docker Compose:

```bash
docker-compose up -d
```
### 4ï¸âƒ£ Verify Containers

Check that all required containers are running:

```bash
docker ps
```
```
Ensure the following containers are running:

- `airflow_webserver`
- `airflow_scheduler`
- `etl-service`
- `postgres`
```

## ğŸš€ DAG Execution Steps
```
1. Open the Airflow UI  
   ğŸ‘‰ http://localhost:8080

2. Enable the DAG:
   - `ecommerce_analytics_pipeline`

3. Trigger the DAG manually â–¶ï¸

4. Confirm:
   - All tasks turn **GREEN**
   - DAG run status = **SUCCESS**

---
```
## âœ… DAG Configuration
```
| Setting       | Value        |
|--------------|--------------|
| Schedule     | `@daily`     |
| Retries      | `2`          |
| Retry Delay  | `5 minutes`  |
| Catchup      | `False`      |
```

## ğŸ” Validation Strategy (Great Expectations)

### âœ” Raw Data Validation
```
- Column presence checks
- Schema consistency
- Executed via **Great Expectations checkpoint**
```
### âœ” Transformed Data Validation
```
- Schema integrity checks
- Data consistency checks
```
### âœ” Failure Handling
```
- DAG fails immediately if validation fails
- Downstream tasks are blocked
```

## ğŸ§ª Unit Testing

Run tests inside the ETL container:

```bash
docker-compose exec etl-service pytest
```
### Included Tests
```
- Transformation logic test
- Schema validation test

âœ” Passing tests ensure reliable ETL logic
```

## ğŸ—‚ Screenshots (Evidence)

### ğŸ“ screenshots/
```

| Screenshot | Description |
|----------|-------------|
| Screenshot_5_docker_ps_running.png | All containers running |
| Screenshot_6_pytest_success.png | Pytest success |
| Screenshot_1_Data_Docs_Validation_Result.png | Great Expectations Data Docs |
| Screenshot_2_Raw_Data_Suite_Detail.png | Raw data expectation suite |
| Screenshot_3_Checkpoint_Run_Success.png | Checkpoint run success |
| Screenshot_4_ge_checkpoint_cli.png | Great Expectations CLI checkpoint run |
```
```
ğŸ“ screenshots/dags_screenshots/
Screenshot	Description
01_airflow_dags_page.png	DAG list
02_dag_grid_success.png	DAG grid success
03_dag_graph_view.png	DAG graph view
04_dag_run_details.png	DAG run details
05_task_log_success.png	Task log output
06_dag_code_file.png	DAG code file
```

â­ **Includes mandatory + bonus screenshots**
```
---

## ğŸ—„ How to Verify SQLite Database

Enter the ETL container:

```bash
docker-compose exec etl-service bash
```
Open the SQLite database:

```bash
sqlite3 /data/analytics.db
```
List tables:

```sql
.tables
```
### Preview Data

```sql
SELECT * FROM analytics_table LIMIT 5;
```
```

## ğŸ Final Status
```
- âœ” Fully containerized
- âœ” Automated data validation implemented
- âœ” Unit test coverage added
- âœ” End-to-end execution verified
- âœ” Portfolio-ready project
```

## ğŸ™Œ Author
```
**Chopra Lakshmi Sathvika**  
Data Engineering | Apache Airflow | Great Expectations | Docker
```
